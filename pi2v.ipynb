{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52cbb85",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Loading wikipedia bios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4c5e25",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## loading raw identities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a2eb59",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "df = pd.read_csv('FinalDataFrame5.csv')\n",
    "df.identities = df.identities.apply(literal_eval)\n",
    "bios = list(df['identities'])\n",
    "\n",
    "bios[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb456f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The rest is similar to twitter bios approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a048cc",
   "metadata": {},
   "source": [
    "# Loading twitter bios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b010d",
   "metadata": {},
   "source": [
    "## load all raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a19f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/user/smadani/navid/data/pis2020.pkl', 'rb') as f:\n",
    "    bios = pickle.load(f)\n",
    "    \n",
    "bios[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc70cc9",
   "metadata": {},
   "source": [
    "## PI frequency and top PIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43e3926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cntr = Counter()\n",
    "\n",
    "for bio in bios:\n",
    "    cntr.update(bio)\n",
    "\n",
    "print(len(bios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b0aa7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "freqs = list(cntr.values())\n",
    "print(f\"percentile freq: {np.percentile(freqs, 99)} mean freq: {np.mean(freqs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b48510dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_pis = {k:v for k,v in cntr.items() if v > 500}\n",
    "len(most_frequent_pis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12b0dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'pi': list(most_frequent_pis.keys()), 'cnt': list(most_frequent_pis.values())})\n",
    "df = df.sort_values(by=['cnt'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac5a2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('twitter_most_frequent_pis.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c6460",
   "metadata": {},
   "source": [
    "### most frequent neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04aeea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "neighbor_cnt = {}\n",
    "\n",
    "for bio in tqdm(bios):\n",
    "    for pi in bio:\n",
    "        if pi in most_frequent_pis:\n",
    "            if pi not in neighbor_cnt:\n",
    "                neighbor_cnt[pi] = Counter()\n",
    "                \n",
    "            rest = [b for b in bio if b!=pi and b in most_frequent_pis]\n",
    "            neighbor_cnt[pi].update(rest)\n",
    "\n",
    "print(len(neighbor_cnt))            \n",
    "\n",
    "#post processing and pruning empty adjacencies\n",
    "\n",
    "for pi, adj in neighbor_cnt.copy().items():\n",
    "    if len(adj) < 2:\n",
    "        neighbor_cnt.pop(pi)\n",
    "    \n",
    "print(f\"size after pruning: {len(neighbor_cnt)}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a59e0cb",
   "metadata": {},
   "source": [
    "### calculating tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ac1acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pi, neighs in tqdm(neighbor_cnt.items()):\n",
    "    for phrase in neighs.keys():\n",
    "        neighs[phrase] /= most_frequent_pis[phrase]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d69a95",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### calculating using bi-partite method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37077142",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy import sparse, io\n",
    "\n",
    "pi_idx = {}\n",
    "for pi in most_frequent_pis:\n",
    "    pi_idx[pi] = len(pi_idx)\n",
    "\n",
    "#creating bipartite matrix\n",
    "usr_pi = []\n",
    "\n",
    "for bio in bios:\n",
    "    cur_usr_pis = []\n",
    "    for pi in bio:\n",
    "        if pi in most_frequent_pis:\n",
    "            cur_usr_pis.append(pi_idx[pi])\n",
    "    if len(cur_usr_pis) < 2:\n",
    "        continue\n",
    "    usr_pi.append(cur_usr_pis)\n",
    "    \n",
    "\n",
    "print(f\"original users: {len(bios)}\")\n",
    "print(f\"no of users after pruning: {len(usr_pi)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae185fc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "usrs = []\n",
    "pis = []\n",
    "scores = []\n",
    "\n",
    "for uid, pis in enumerate(usr_pi):\n",
    "    for pi in pis:\n",
    "        usrs.append(uid)\n",
    "        pis.append(pi)\n",
    "        scores.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf9be0f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%% time\n",
    "\n",
    "BP_MATRIX_FILENAME = \"./bipartite_pi.mtx\"\n",
    "output_matrix = sparse.coo_matrix((scores, (usrs, pis)))\n",
    "io.mmwrite(BP_MATRIX_FILENAME, output_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72cad62",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!du -hs ./bipartite_pi.mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12192548",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "\n",
    "io.mmread(BP_MATRIX_FILENAME, output_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44744b56",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3123d499",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!rm ./bipartite_pi.mtx.gz\n",
    "!gzip ./bipartite_pi.mtx\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6010f65c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../bipartite-pairs/python-scoring/\")\n",
    "import score_data\n",
    "\n",
    "BP_SCORING_OUTPUT = './bipartite_output.csv.gz'\n",
    "score_data.score_only(\n",
    "    BP_MATRIX_FILENAME+\".gz\",\n",
    "    ['weighted_corr_exp'],\n",
    "    BP_SCORING_OUTPUT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698bdfc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!zcat ./bipartite_output.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94224328",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "BP_SCORING_OUTPUT = './bipartite_output.csv.gz'\n",
    "df = pd.read_csv(BP_SCORING_OUTPUT)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0fdd88",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# recreate the neighboring dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe12e66c",
   "metadata": {},
   "source": [
    "### save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b66c7124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pis = []\n",
    "positives = []\n",
    "negatives = []\n",
    "\n",
    "for pi, cntr in tqdm(neighbor_cnt.items()):\n",
    "    cur_neg = [x for x in most_frequent_pis if x not in neighbor_cnt[pi]]\n",
    "    if len(cur_neg) > 20:\n",
    "        cur_neg = list(np.random.choice(cur_neg, size=20, replace=False))\n",
    "    cur_pos = [x[0] for x in neighbor_cnt[pi].most_common(5)]\n",
    "    if len(cur_neg) < 4 or len(neighbor_cnt[pi])<2:\n",
    "        print(f\"PASSING PI: {pi}\")\n",
    "        continue\n",
    "    positives.append(cur_pos)\n",
    "    pis.append(pi)\n",
    "    negatives.append(cur_neg)\n",
    "                 \n",
    "print(f\"saving {len(pis)} pis\")\n",
    "df = pd.DataFrame({'pis': pis, 'positives': positives, 'negatives': negatives})\n",
    "df.to_csv('twitter_pi_with_neighbors_tfidf.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eea41545",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l wiki_pi_with_neighbors_standard.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cdb1fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head wiki_pi_with_neighbors_standard.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e553a1",
   "metadata": {},
   "source": [
    "## create test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0e18435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(bios, test_size=0.01, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "407be37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/test_bios.pkl', 'wb') as f:\n",
    "    pickle.dump(test, f)\n",
    "    \n",
    "with open('./data/train_bios.pkl', 'wb') as f:\n",
    "    pickle.dump(train, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf8c10",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24388589",
   "metadata": {},
   "source": [
    "## phrase cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008d8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/train_bios.pkl', 'rb') as f:\n",
    "    bios = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ea45e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a vocabulary of phrases\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "\n",
    "pi_cnt = Counter()\n",
    "for bio in tqdm(bios):\n",
    "    pi_cnt.update(bio)\n",
    "\n",
    "len(pi_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ad7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_cnt.most_common(len(pi_cnt))[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89587ee3",
   "metadata": {},
   "source": [
    "## cleaning each bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5283dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# phrases of lenght at least 2\n",
    "# profiles with at least 2 phrases\n",
    "# pis that's been repeated at least 10 times in dataset\n",
    "\n",
    "def clean_pis(all_pis):\n",
    "    result = []\n",
    "    for pis in tqdm(all_pis):\n",
    "        current_pi = set()\n",
    "        for pi in pis:\n",
    "            if len(pi) >= 2 and pi_cnt[pi] >= 10:\n",
    "                current_pi.add(pi)\n",
    "        if len(current_pi) > 1:\n",
    "            result.append(list(current_pi))\n",
    "            \n",
    "    return result\n",
    "            \n",
    "cleaned_bios = clean_pis(bios)\n",
    "print(len(cleaned_bios))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8579a27",
   "metadata": {},
   "source": [
    "# Contrastive learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4049e62",
   "metadata": {},
   "source": [
    "## generating positive negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from each person's bio I create at most K triplets\n",
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "def pair_in_list(current_pair, l):\n",
    "    for pair in l:\n",
    "        if current_pair[0] in pair and current_pair[1] in pair:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def generate_triplets(bios, k=3):\n",
    "    samples = []\n",
    "    for idx, bio in tqdm(enumerate(bios), total=len(bios)):\n",
    "        iters = min(len(bio)-1, k)\n",
    "        chosen_pis = []\n",
    "        for i in range(iters):\n",
    "            pos1, pos2 = np.random.choice(bio, size=2, replace=False)\n",
    "            while pair_in_list([pos1,pos2], chosen_pis):\n",
    "                pos1, pos2 = np.random.choice(bio, size=2, replace=False)\n",
    "                #print(pos1, pos2, chosen_pis, len(bio), bio)\n",
    "            chosen_pis.append([pos1, pos2])\n",
    "            neg_idx = randint(0, len(bios)-1)\n",
    "            neg_sample = np.random.choice(bios[neg_idx], size=1)[0]\n",
    "            samples.append([pos1, pos2, neg_sample])\n",
    "    return samples\n",
    "            \n",
    "triplets = generate_triplets(cleaned_bios, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef2d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(triplets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d18ff5",
   "metadata": {},
   "source": [
    "## save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('/user/smadani/navid/data/triplets.pkl', 'wb') as f:\n",
    "    pickle.dump(triplets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b7a0d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('data/triplets.pkl', 'rb') as f:\n",
    "    triplets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e7dbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, valid_set = train_test_split(triplets, test_size=0.01, shuffle=True)\n",
    "train_set, test_set = train_test_split(train_set, test_size=0.01, shuffle=True)\n",
    "\n",
    "print(f\"train size: {len(train_set)}, validation size: {len(valid_set)}, test size: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9209cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# writing the data into the file\n",
    "with open('data/valid.csv', 'w') as f:   \n",
    "    write = csv.writer(f, delimiter='\\t')\n",
    "    write.writerows(valid_set)\n",
    "    \n",
    "with open('data/train.csv', 'w') as f:   \n",
    "    write = csv.writer(f, delimiter='\\t')\n",
    "    write.writerows(train_set)\n",
    "\n",
    "with open('data/test.pckl', 'wb') as f:   \n",
    "    pickle.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba7ad2d",
   "metadata": {},
   "source": [
    "## finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf22515a",
   "metadata": {},
   "source": [
    "### building the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d8c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, util\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "with open('data/train.csv', newline='') as f:\n",
    "    train_examples = []\n",
    "    reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in tqdm(reader):\n",
    "        train_examples.append(InputExample(texts=[row[0], row[1]], label=1.0))\n",
    "        train_examples.append(InputExample(texts=[row[0], row[2]], label=0.0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca3a11",
   "metadata": {},
   "source": [
    "### loading evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f1ae891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import evaluation\n",
    "\n",
    "with open('data/valid.csv', newline='') as f:\n",
    "    sent1s = []\n",
    "    sent2s = []\n",
    "    scores = []\n",
    "    i = 0\n",
    "    reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in tqdm(reader):\n",
    "        sent1s.append(row[0])\n",
    "        sent1s.append(row[0])\n",
    "        sent2s.append(row[1])\n",
    "        sent2s.append(row[2])\n",
    "        scores.append(1.0)\n",
    "        scores.append(0.0)\n",
    "        i += 1\n",
    "evaluator = evaluation.EmbeddingSimilarityEvaluator(sent1s, sent2s, scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645e1e0",
   "metadata": {},
   "source": [
    "### creating data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "253e199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "from torch import nn\n",
    "\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "dense_model = models.Dense(in_features=model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "model.add_module('3', dense_model)\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=128)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6804d7",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c30365eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_path = 'models/miniLM-L6-finetuned'\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=5,\n",
    "          evaluation_steps=10000,\n",
    "          warmup_steps=10000,\n",
    "          output_path=output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67356ab1",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f996d9b",
   "metadata": {},
   "source": [
    "### loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99293b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82ba41ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "model = SentenceTransformer('./models/miniLM-L6-finetuned/')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee894bf",
   "metadata": {},
   "source": [
    "### calculating encodings for all phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad34c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pis = set()\n",
    "\n",
    "for bio in cleaned_bios:\n",
    "    pis.update(bio)\n",
    "\n",
    "pis = list(pis)\n",
    "print(len(pis))\n",
    "\n",
    "embeddings = model.encode(pis, convert_to_tensor=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "237ba151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(pi, all_pis, all_pi_embs, model, k=11):\n",
    "    cur_emb = model.encode(pi, convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(cur_emb, all_pi_embs).detach().cpu().numpy()[0]\n",
    "    most_similars = np.argsort(cosine_scores)[-k:]\n",
    "    return [(all_pis[i], cosine_scores[i]) for i in most_similars if pi!=all_pis[i]]\n",
    "\n",
    "most_similar('mima', pis, embeddings, model, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94edeb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(w1, w2, model=model):\n",
    "    emb1 = model.encode(w1, convert_to_tensor=True)\n",
    "    emb2 = model.encode(w2, convert_to_tensor=True)\n",
    "    return util.cos_sim(emb1, emb2)\n",
    "\n",
    "print(\n",
    "    get_similarity('isfj', 'man'),\n",
    "    get_similarity('isfj', 'woman'),\n",
    "    get_similarity('isfj', 'man', model=model),\n",
    "    get_similarity('isfj', 'woman', model=model),\n",
    ")\n",
    "\n",
    "print(\n",
    "    get_similarity('intj', 'man'),\n",
    "    get_similarity('intj', 'woman'),\n",
    "    get_similarity('intj', 'man', model=model),\n",
    "    get_similarity('intj', 'woman', model=model),\n",
    ")\n",
    "\n",
    "print(\n",
    "    get_similarity('entj', 'man'),\n",
    "    get_similarity('entj', 'woman'),\n",
    "    get_similarity('entj', 'man', model=model),\n",
    "    get_similarity('entj', 'woman', model=model),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8a1f8c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mom_emb = base_model.encode('intp', convert_to_tensor=True)\n",
    "dad_emb = base_model.encode('esfj', convert_to_tensor=True)\n",
    "util.cos_sim(mom_emb, dad_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821f8ae",
   "metadata": {},
   "source": [
    "### loading not tuned model and doing the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ffd31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "base_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "base_embs = base_model.encode(pis, convert_to_tensor=True)\n",
    "base_cosine_scores = util.cos_sim(base_embs, base_embs).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84054f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "target_word = 'build the wall'\n",
    "df1 = pd.DataFrame(most_similar(target_word, pis, embeddings, model), columns=['identifier', 'similarity'])\n",
    "df1['model'] = 'fine-tuned-sentence-bert'\n",
    "df2 = pd.DataFrame(most_similar(target_word, pis, base_embs, base_model), columns=['identifier', 'similarity'])\n",
    "df2['model'] = 'original-sentence-bert'\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\n",
    "\n",
    "ax1.scatter(x=df1['identifier'], y=df1['similarity'])\n",
    "ax1.tick_params(axis='x', rotation=-60)\n",
    "ax1.set_xlabel('phrase')\n",
    "ax1.set_ylabel('similarity')\n",
    "ax1.set_title('fine-tuned-sentence-bert')\n",
    "\n",
    "\n",
    "ax2.scatter(x=df2['identifier'], y=df2['similarity'])\n",
    "ax2.tick_params(axis='x', rotation=-60)\n",
    "ax2.set_xlabel('phrase')\n",
    "ax2.set_ylabel('similarity')\n",
    "ax2.set_title('original-sentence-bert')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad9278e",
   "metadata": {},
   "source": [
    "### analyzing personalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "22507a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for personality in ['ESTJ', 'ENTJ', 'ESFJ', 'ENFJ', 'ISTJ', 'ISFJ', 'INTJ', 'INFJ', 'ESTP', 'ESFP', 'ENTP', 'ENFP', 'ISTP', 'ISFP', 'INTP', 'INFP']:\n",
    "    if personality in pis or personality.lower() in pis:\n",
    "        print(f\"{personality}: True\")\n",
    "    else:\n",
    "        print(f\"{personality}: False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2c1b63b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "personalities = ['ESTJ', 'ENTJ', 'ESFJ', 'ENFJ', 'ISTJ', 'ISFJ', 'INTJ', 'INFJ', 'ESTP', 'ESFP', 'ENTP', 'ENFP', 'ISTP', 'ISFP', 'INTP', 'INFP']\n",
    "personalities = [p.lower() for p in personalities]\n",
    "\n",
    "pers_emb = model.encode(personalities, convert_to_tensor=True)\n",
    "pers_emb_base = base_model.encode(personalities, convert_to_tensor=True)\n",
    "\n",
    "base_cosine_scores = util.cos_sim(pers_emb_base, pers_emb_base).detach().cpu().numpy()\n",
    "cosine_scores = util.cos_sim(pers_emb, pers_emb).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "406d3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.heatmap(cosine_scores)\n",
    "ax.set_xticklabels(personalities, rotation=90)\n",
    "ax.set_yticklabels(personalities, rotation=0)\n",
    "\n",
    "# plt.xticks(ticks=personalities)\n",
    "# plt.yticks(ticks=personalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "db1ab792",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = sns.heatmap(base_cosine_scores)\n",
    "ax.set_xticklabels(personalities, rotation=90)\n",
    "ax.set_yticklabels(personalities, rotation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c00590e",
   "metadata": {},
   "source": [
    "### comparing in gensim vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f40dfb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v = api.load(\"glove-wiki-gigaword-50\")\n",
    "w2v.most_similar(\"glass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "636e7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(w2v.key_to_index.keys())\n",
    "\n",
    "vocab_embs = model.encode(vocab, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "036f73ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar('vaccine', vocab, vocab_embs, model, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "8b650b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar('blm', vocab, vocab_embs, model, k=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679f9e7",
   "metadata": {},
   "source": [
    "# Word2vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bee5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('/user/smadani/navid/data/pis2020.pkl', 'rb') as f:\n",
    "    bios = pickle.load(f)\n",
    "    \n",
    "bios[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f51047e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(len(bios))\n",
    "\n",
    "class Callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subed = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
    "        self.epoch += 1\n",
    "\n",
    "monitor = Callback()\n",
    "model = Word2Vec(bios, vector_size=256, window=5, min_count=1,\n",
    "                 negative=10, workers=30, epochs=100, callbacks=[monitor],\n",
    "                 compute_loss=True)\n",
    "\n",
    "model.save('./models/w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc4c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"./models/w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65904ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('he', topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc7b98",
   "metadata": {},
   "source": [
    "# Downstream tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a84e0e",
   "metadata": {},
   "source": [
    "## hold-one-out prediction of PIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43527519",
   "metadata": {},
   "source": [
    "### clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "127e7647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/test_bios.pkl', 'rb') as f:\n",
    "    test_bios = pickle.load(f)\n",
    "\n",
    "with open('data/train_bios.pkl', 'rb') as f:\n",
    "    train_bios = pickle.load(f)\n",
    "    \n",
    "all_bios = train_bios + test_bios\n",
    "print(len(all_bios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3231d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a vocabulary of phrases\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "\n",
    "pi_cnt = Counter()\n",
    "for bio in tqdm(all_bios):\n",
    "    pi_cnt.update(bio)\n",
    "\n",
    "len(pi_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca3cb98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# phrases of lenght at least 2\n",
    "# profiles with at least 2 phrases\n",
    "# pis that's been repeated at least 10 times in dataset\n",
    "\n",
    "def clean_pis(all_pis):\n",
    "    result = []\n",
    "    for pis in tqdm(all_pis):\n",
    "        current_pi = set()\n",
    "        for pi in pis:\n",
    "            if len(pi) >= 2 and pi_cnt[pi] >= 10:\n",
    "                current_pi.add(pi)\n",
    "        if len(current_pi) > 1:\n",
    "            result.append(list(current_pi))\n",
    "            \n",
    "    return result\n",
    "            \n",
    "cleaned_all_bios = clean_pis(all_bios)\n",
    "cleaned_test_bios = clean_pis(test_bios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6eed8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_all_bios[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786fbb9",
   "metadata": {},
   "source": [
    "### create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa5f11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_ds = []\n",
    "\n",
    "\n",
    "for bio in cleaned_test_bios:\n",
    "    hold_out_idx = np.random.randint(0, len(bio))\n",
    "    remaining = [x for i, x in enumerate(bio) if i != hold_out_idx]\n",
    "    remaining = ', '.join(remaining)\n",
    "    target = bio[hold_out_idx]\n",
    "    \n",
    "    test_ds.append((remaining, target))\n",
    "\n",
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5abb2cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pis = set()\n",
    "for bio in cleaned_all_bios:\n",
    "    for pi in bio:\n",
    "        all_pis.add(pi)\n",
    "\n",
    "all_pis = list(all_pis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0fb6b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_pis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "62e3452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "pi_dict = OrderedDict()\n",
    "for p in all_pis:\n",
    "    pi_dict[p] = len(pi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ed23ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pi_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73d94c",
   "metadata": {},
   "source": [
    "### load original sentence bert and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b41b7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_x, bio_y = zip(*test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "19d54816",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models, util\n",
    "\n",
    "orig_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "orig_emb_x = orig_model.encode(bio_x, convert_to_tensor=True)\n",
    "orig_emb_all = orig_model.encode(all_pis, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b73357",
   "metadata": {},
   "source": [
    "### calculate rank score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead525b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x, y in tqdm(zip(orig_emb_x, bio_y)):\n",
    "    cosine_scores = util.cos_sim(x, orig_emb_all).detach().cpu().numpy()[0]\n",
    "    target_idx = pi_dict[y]\n",
    "    argsort = np.argsort(cosine_scores)\n",
    "    for i, arg in enumerate(argsort):\n",
    "        if arg == target_idx:\n",
    "            rank = len(argsort)-i\n",
    "            ranks.append(rank)\n",
    "            break\n",
    "        if i == len(argsort)-1:\n",
    "            print(y)\n",
    "            print(\"element not available in dict!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8a9d9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rank(x_emb, y, orig_emb_all):\n",
    "    cosine_scores = util.cos_sim(x_emb, orig_emb_all)[0]\n",
    "    target_idx = pi_dict[y]\n",
    "    argsort = np.argsort(cosine_scores)\n",
    "    for i, arg in enumerate(argsort):\n",
    "        if arg == target_idx:\n",
    "            rank = len(argsort)-i\n",
    "            return rank\n",
    "        if i == len(argsort)-1:\n",
    "            raise Exception()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5b0f9400",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_emb_all_np = orig_emb_all.detach().cpu().numpy()\n",
    "orig_x_emb_np = orig_emb_x.detach().cpu().numpy()\n",
    "bio_y_ = bio_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7e891657",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "res = Parallel(n_jobs=30)(delayed(calculate_rank)(x_emb, y, orig_emb_all_np) for x_emb, y in zip(orig_x_emb_np, bio_y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "25cbc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([r for r in res if r < 20]), len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4dc40",
   "metadata": {},
   "source": [
    "### load finetuned sentence bert and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7d09d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_x, bio_y = zip(*test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d967a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models, util\n",
    "\n",
    "fint_model = SentenceTransformer('./models/miniLM-L6-finetuned/')\n",
    "\n",
    "fint_emb_x = fint_model.encode(bio_x, convert_to_tensor=True)\n",
    "fint_emb_all = fint_model.encode(all_pis, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e57a29",
   "metadata": {},
   "source": [
    "### calculate rank score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed8b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x, y in tqdm(zip(fint_emb_x, bio_y)):\n",
    "    cosine_scores = util.cos_sim(x, fint_emb_all).detach().cpu().numpy()[0]\n",
    "    target_idx = pi_dict[y]\n",
    "    argsort = np.argsort(cosine_scores)\n",
    "    for i, arg in enumerate(argsort):\n",
    "        if arg == target_idx:\n",
    "            rank = len(argsort)-i\n",
    "            ranks.append(rank)\n",
    "            break\n",
    "        if i == len(argsort)-1:\n",
    "            print(y)\n",
    "            print(\"element not available in dict!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "90d91438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rank(x_emb, y, fint_emb_all):\n",
    "    cosine_scores = util.cos_sim(x_emb, fint_emb_all)[0]\n",
    "    target_idx = pi_dict[y]\n",
    "    argsort = np.argsort(cosine_scores)\n",
    "    for i, arg in enumerate(argsort):\n",
    "        if arg == target_idx:\n",
    "            rank = len(argsort)-i\n",
    "            return rank\n",
    "        if i == len(argsort)-1:\n",
    "            raise Exception()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "82681bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fint_emb_all_np = fint_emb_all.detach().cpu().numpy()\n",
    "fint_emb_x_np = fint_emb_x.detach().cpu().numpy()\n",
    "bio_y_ = bio_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "15fdfed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "fin_ranks = Parallel(n_jobs=30)(delayed(calculate_rank)(x_emb, y, fint_emb_all_np) for x_emb, y in zip(fint_emb_x_np, bio_y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8c4695da",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([r for r in fin_ranks if r < 1000]), len(fin_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a7c8a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([r for r in res if r < 1000]), len(fin_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0545914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(fin_ranks), np.median(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe137609",
   "metadata": {},
   "source": [
    "# build survey questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8af9de",
   "metadata": {},
   "source": [
    "## neighborhood score + negative sampling choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5d522613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "df = pd.read_csv('twitter_pi_with_neighbors_tfidf.csv')\n",
    "print(df.head())\n",
    "df.positives = df.positives.apply(literal_eval)\n",
    "df.negatives = df.negatives.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd980929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from random import randint\n",
    "\n",
    "pis = df['pis']\n",
    "positives = df['positives']\n",
    "negatives = df['negatives']\n",
    "\n",
    "sample_cnt = 500\n",
    "questions = []\n",
    "targets = []\n",
    "other_choices = []\n",
    "\n",
    "sample_idices = np.random.randint(0, len(df), size=sample_cnt)\n",
    "\n",
    "for qid in sample_idices:\n",
    "    q = pis[qid]\n",
    "    cur_pos = positives[qid]\n",
    "    cur_neg = negatives[qid]\n",
    "    target = cur_pos[randint(0,len(cur_pos)-1)]\n",
    "    targets.append(target)\n",
    "    questions.append(q)\n",
    "    other_choices.append(np.random.choice(cur_neg, size=3, replace=False))\n",
    "    \n",
    "res = pd.DataFrame({'question_pi': questions, 'ans_pi': targets, 'other_choices': other_choices})\n",
    "res.to_csv('surrvey_tfidf_twitter.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b86922b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head surrvey_tfidf_twitter.csv -n 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e799829b",
   "metadata": {},
   "source": [
    "## model based question generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3166fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models, util\n",
    "\n",
    "fint_model = SentenceTransformer('./models/miniLM-L6-finetuned/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ac0a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from random import randint\n",
    "\n",
    "\n",
    "all_pis = df['pis']\n",
    "sims = df['similars']\n",
    "sample_cnt = 500\n",
    "\n",
    "questions = []\n",
    "targets = []\n",
    "other_choices = []\n",
    "\n",
    "i = 0\n",
    "while i < sample_cnt:\n",
    "    idx = randint(0, len(all_pis)-1)\n",
    "    questions.append(all_pis[idx])\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88ea973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fint_emb_x = fint_model.encode(questions, convert_to_tensor=True)\n",
    "fint_emb_all = fint_model.encode(all_pis, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43b25cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "targets = []\n",
    "other_choices = []\n",
    "\n",
    "for x in tqdm(fint_emb_x):\n",
    "    cosine_scores = util.cos_sim(x, fint_emb_all).detach().cpu().numpy()[0]\n",
    "    argsort = np.argsort(cosine_scores)\n",
    "    best_k = argsort[-6:-1]\n",
    "    worst_k = argsort[:len(argsort)//2]\n",
    "    \n",
    "    target_idx = np.random.choice(best_k)\n",
    "    targets.append(all_pis[target_idx])\n",
    "    \n",
    "    other_idxs = np.random.choice(worst_k, size=3, replace=False)\n",
    "    other_choices.append([all_pis[x] for x in other_idxs])\n",
    "    \n",
    "    \n",
    "res = pd.DataFrame({'question_pi': questions, 'ans_pi': targets, 'other_choices': other_choices})\n",
    "res.to_csv('modelbased-selection.csv', index=False, header=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf38c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head modelbased-selection.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae77b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.075px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
